{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images():\n",
    "    \"\"\"\n",
    "    read images in images folder\n",
    "    return: images, filenames as numpy array \n",
    "    \"\"\"\n",
    "    files = os.listdir('images/')\n",
    "    imageList = []\n",
    "    fileNames = []\n",
    "    for file in files:\n",
    "        image = cv2.imread(f\"images/{file}\", 0)\n",
    "        if image.shape == (350, 350):\n",
    "            imageList.append(image)\n",
    "            fileNames.append(file)\n",
    "    return np.array(imageList), np.array(fileNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_legend():\n",
    "    \"\"\"\n",
    "    read legend of images from data/lenged.csv\n",
    "    return lengend data, list of target values\n",
    "    \"\"\"\n",
    "    df = pd.read_csv('data/legend.csv')\n",
    "    df['emotion'] = df['emotion'].str.lower()\n",
    "    label_encoder = LabelEncoder()\n",
    "    emotion = df['emotion']\n",
    "    integer_encoded = label_encoder.fit_transform(emotion)\n",
    "    df['emotion_encoded'] = integer_encoded\n",
    "    \n",
    "    return df, label_encoder.inverse_transform([0,1,2,3,4,5,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_unmatched_image(imgs, fileNames, df): \n",
    "    y = []\n",
    "    for file in fileNames:\n",
    "        value = df[df['image'] == file].emotion_encoded.values\n",
    "        if len(value) == 1:\n",
    "            y.append(value[0])\n",
    "        else:\n",
    "            y.append(-1)\n",
    "    y_tmp = np.array(y)\n",
    "    deleteList = np.where(y_tmp == -1)[0]\n",
    "    images = np.delete(imgs, deleteList, 0)\n",
    "    y_target = np.delete(y_tmp, deleteList)\n",
    "    file_names = np.delete(fileNames, deleteList)\n",
    "    \n",
    "    return images, y_target, file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process():\n",
    "    imgs, fileNames = read_images()\n",
    "    df, target_names = read_legend()\n",
    "    images, y_target, file_names = delete_unmatched_image(imgs, fileNames, df)\n",
    "    y_target=pd.get_dummies(y_target)\n",
    "    \n",
    "    return images, y_target, target_names, file_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_many_hdf5(images, target):\n",
    "    file = h5py.File('h5/images6.h5', 'w')\n",
    "    dataset = file.create_dataset('images', np.shape(images), data=images)\n",
    "    output = file.create_dataset('target', np.shape(target), data=target)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "def has_face(img_path):\n",
    "    files = os.listdir(img_path)\n",
    "    total = len(files)\n",
    "    count = 0;\n",
    "    not_face = []\n",
    "    for file in files:\n",
    "        img = cv2.imread(f'{img_path}/{file}')\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.01, 0)\n",
    "        if len(faces) > 0:\n",
    "            count += 1\n",
    "        else:\n",
    "            not_face.append(file)\n",
    "            os.remove(f'{img_path}/{file}')\n",
    "    return count, total, not_face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5154, 5154, [])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "has_face('fer13_aug/train_augumentation/angry/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['angry', 'disgust','fear','happy','neutral','sad','surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emo in emotions:\n",
    "    has_face(f'fer13_aug/validation_augumentation/{emo}/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
